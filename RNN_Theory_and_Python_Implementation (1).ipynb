{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI4yQ-On3NUu"
      },
      "source": [
        "# RNN (Recurrent Neural Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvOTuIXO3NU0"
      },
      "source": [
        "## 1)  After CNN, why RNN is there?\n",
        "\n",
        "The backpropagation algorithm in CNN (convolutional neural network), we know that their output only considers the influence of the previous input and does not consider the influence of other moments of input, such as simple cats, dogs, handwritten numbers and other single objects.\n",
        "\n",
        "However, for some related to time, such as the prediction of the next moment of the video, the prediction of the content of the previous and subsequent documents, etc., the performance of these algorithms is not satisfactory. Therefore, RNN should be applied and was born."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIXpuDJF3NU1"
      },
      "source": [
        "## 2) What is RNN?\n",
        "\n",
        "RNN is a special neural network structure, which is proposed based on the view that \" human cognition is based on past experience and memory \". It is different from DNN and CNN in that it not only considers the input of the previous moment, And it gives the network a 'memory' function of the previous content .\n",
        "\n",
        "The reason why RNN is called recurrent neural network is that the current output of a sequence is also related to the previous output. The specific manifestation is that the network memorizes the previous information and applies it to the current output calculation, that is, the nodes between the hidden layers are connected, and the input of the hidden layer includes not only the output of the input layer It also includes the output of the hidden layer from the previous moment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvNcXz9T3NU2"
      },
      "source": [
        "## 3) What are the main application areas of RNN ?\n",
        "\n",
        "There are many application fields of RNN. It can be said that as long as the problem of chronological order is considered, RNN can be used to solve it. Here are some common application fields:\n",
        "\n",
        "    ① Natural Language Processing (NLP) : There are video processing ,  text generation , language model , image processing\n",
        "\n",
        "    ② Machine translation , machine writing novels\n",
        "\n",
        "    ③ Speech recognition\n",
        "\n",
        "    ④ Image description generation\n",
        "\n",
        "    ⑤ Text similarity calculation\n",
        "\n",
        "    ⑥ New application areas such as music recommendation , Netease koala product recommendation , Youtube video recommendation, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D0m_Yw_3NU3"
      },
      "source": [
        "### Different types of RNN\n",
        "\n",
        "![alt](https://miro.medium.com/max/1400/0*1PKOwfxLIg_64TAO.jpeg)\n",
        "\n",
        "#### One-to-one:\n",
        "\n",
        "This also called as Plain/Vaniall Neural networks. It deals with Fixed size of input to Fixed size of Output where they are independent of previous information/output.\n",
        "\n",
        "Ex: Image classification.\n",
        "\n",
        "\n",
        "#### One-to-Many:\n",
        "\n",
        "it deals with fixed size of information as input that gives sequence of data as output.\n",
        "\n",
        "Ex:Image Captioning takes image as input and outputs a sentence of words.\n",
        "\n",
        "![alt](https://miro.medium.com/max/1400/0*d9FisCKzVZ29SxUu.png)\n",
        "\n",
        "#### Many-to-One:\n",
        "\n",
        "It takes Sequence of information as input and ouputs a fixed size of output.\n",
        "\n",
        "Ex:sentiment analysis where a given sentence is classified as expressing positive or negative sentiment.\n",
        "\n",
        "\n",
        "#### Many-to-Many:\n",
        "\n",
        "It takes a Sequence of information as input and process it recurrently outputs a Sequence of data.\n",
        "\n",
        "Ex: Machine Translation, where an RNN reads a sentence in English and then outputs a sentence in French."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTycGjt73NU4"
      },
      "source": [
        "## RNN model structure\n",
        "\n",
        "Earlier we said that RNN has the function of \"memory\" of time, so how does it realize the so-called \"memory\"?\n",
        "\n",
        "![alt](img/1.png)\n",
        "\n",
        "<center>Figure 1 RNN structure diagram </center>\n",
        "\n",
        "As shown in Figure 1, we can see that the RNN hierarchy is simpler than CNN.It mainly consists of an input layer , a Hidden Layer , and an output layer .\n",
        "\n",
        "And you will find that **there is an arrow in the Hidden Layer to  indicate the cyclic update of the data.This is the method to implement the time memory function.**\n",
        "\n",
        "![alt](https://miro.medium.com/max/1400/1*xn5kA92_J5KLaKcP7BMRLA.gif)\n",
        "\n",
        "* t — time step\n",
        "* X — input\n",
        "* h — hidden state\n",
        "* length of X — size/dimension of input\n",
        "* length of h — no. of hidden units.\n",
        "\n",
        "![alt](img/2.png)\n",
        "\n",
        "<center>Figure 2 Unfolded RNN Diagram </center>\n",
        "\n",
        "Figure 2 shows the hierarchical expansion of the Hidden Layer where\n",
        "\n",
        "**T-1, t, t + 1 represent the time series**\n",
        "\n",
        "**X represents the input sample**\n",
        "\n",
        "**St represents the memory of the sample at time t, St = f (W * St -1 + U * Xt).**\n",
        "\n",
        "**W is the weight of the input**\n",
        "\n",
        "**U is the weight of the input sample at the moment**\n",
        "\n",
        "**V is the weight of the output sample.**\n",
        "\n",
        "\n",
        "### Feedforward\n",
        "\n",
        "At t = 1, the general initialization input S0 = 0, randomly initializes W, U, V, and calculates the following formula:\n",
        "\n",
        "![alt](img/21.png)\n",
        "\n",
        "Among them, f and g are activation functions. Among them, f can be tanh, relu, sigmoid and other activation functions, g is usually softmax or other.\n",
        "\n",
        "Time advances, and the state s1 at this time, as the memory state at time 1, will participate in the prediction activity at the next time, that is:\n",
        "\n",
        "![alt](img/22.png)\n",
        "\n",
        "By analogy, you can get the final output value:\n",
        "\n",
        "![alt](img/23.png)\n",
        "\n",
        "\n",
        "Note :\n",
        "\n",
        "1. Here W, U, V are equal at each moment ( weight sharing ).\n",
        "\n",
        "2. The hidden state can be understood as: S = f (existing input + past memory summary )\n",
        "\n",
        "\n",
        "## Back propagation through TIME of RNN\n",
        "\n",
        "Earlier we introduced the forward propagation method of RNN, so how are the weight parameters W, U, and V of the RNN updated?\n",
        "\n",
        "Each output will have a value Ot error value , Et the total error may be expressed as: ![alt](img/31.png)\n",
        "\n",
        " The loss function can use either the cross-entropy loss function or the squared error loss function .\n",
        "\n",
        "Because the output of each step does not only depend on the network of the current step, but also the state of the previous steps, then this modified BP algorithm is called Backpropagation Through Time ( BPTT ), which is the reverse transfer of the error value at the output end. The gradient descent method is updated.\n",
        "\n",
        "That is, the gradient of the parameter is required:\n",
        "\n",
        "![alt](img/32.png)\n",
        "\n",
        " First we solve the update method of W. From the previous update of W, it can be seen that it is the sum of the partial derivatives of the deviations at each moment.\n",
        "\n",
        "Here we take time t = 3 as an example.According to the chain derivation rule, we can get the partial derivative at time t = 3 as:\n",
        "\n",
        "\n",
        "![alt](img/331.png)\n",
        "\n",
        "\n",
        "At this time, according to the formula, ![alt](img/331.png) we will find that in addition to W, S3 is also related to S2 at the previous moment.\n",
        "\n",
        "![alt](https://miro.medium.com/max/1400/0*ENwCVS8XI8cjCy55.jpg)\n",
        "\n",
        "For S3, expand directly to get the following formula:\n",
        "\n",
        "![alt](img/34.png)\n",
        "\n",
        "For S2, expand directly to get the following formula:\n",
        "\n",
        "![alt](img/35.png)\n",
        "\n",
        "For S1, expand directly to get the following formula:\n",
        "\n",
        "![alt](img/36.png)\n",
        "\n",
        "Combine the above three formulas to get:\n",
        "\n",
        "![alt](img/37.png)\n",
        "\n",
        "This gives the formula:\n",
        "\n",
        "![alt](img/381.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBt2t5yW3NU6"
      },
      "source": [
        "What is to be explained here is ![alt](img/39.png)  that it means that S3 directly differentiates W without considering the effect of S2 (that is, for example, y = f (x) * g (x) is the same as the derivative of x)\n",
        "\n",
        "The second is the update method for U. Since the parameter U and W are similar, they will not be described here, and the specific formula finally obtained is as follows:\n",
        "\n",
        "![alt](img/40.png)\n",
        "\n",
        "Finally, give the updated formula of V (V is only related to output O):\n",
        "\n",
        "![alt](img/41.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggBwVWAW3NU7"
      },
      "source": [
        "### Going little deeper\n",
        "\n",
        "Let’s focus on one error term et.\n",
        "\n",
        "You’ve calculated the cost function et, and now you want to propagate your cost function back through the network because you need to update the weights.\n",
        "\n",
        "Essentially, every single neuron that participated in the calculation of the output, associated with this cost function, should have its weight updated in order to minimize that error. And the thing with RNNs is that it’s not just the neurons directly below this output layer that contributed but all of the neurons far back in time. So, you have to propagate all the way back through time to these neurons.\n",
        "\n",
        "The problem relates to updating wrec (weight recurring) – the weight that is used to connect the hidden layers to themselves in the unrolled temporal loop.\n",
        "\n",
        "For instance, to get from xt-3 to xt-2 we multiply xt-3 by wrec. Then, to get from xt-2 to xt-1 we again multiply xt-2 by wrec. So, we multiply with the same exact weight multiple times, and this is where the problem arises: when you multiply something by a small number, your value decreases very quickly.\n",
        "\n",
        "As we know, weights are assigned at the start of the neural network with the random values, which are close to zero, and from there the network trains them up. But, when you start with wrec close to zero and multiply xt, xt-1, xt-2, xt-3, … by this value, your gradient becomes less and less with each multiplication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzDGvC-B3NU8"
      },
      "source": [
        "#### What does this mean for the network?\n",
        "\n",
        "The lower the gradient is, the harder it is for the network to update the weights and the longer it takes to get to the final result.\n",
        "\n",
        "For instance, 1000 epochs might be enough to get the final weight for the time point t, but insufficient for training the weights for the time point t-3 due to a very low gradient at this point. However, the problem is not only that half of the network is not trained properly.\n",
        "\n",
        "The output of the earlier layers is used as the input for the further layers. Thus, the training for the time point t is happening all along based on inputs that are coming from untrained layers. So, because of the vanishing gradient, the whole network is not being trained properly.\n",
        "\n",
        "To sum up, if wrec is small, you have vanishing gradient problem, and if wrec is large, you have exploding gradient problem.\n",
        "\n",
        "For the vanishing gradient problem, the further you go through the network, the lower your gradient is and the harder it is to train the weights, which has a domino effect on all of the further weights throughout the network.\n",
        "\n",
        "That was the main roadblock to using Recurrent Neural Networks. But let’s now check what are the possible solutions to this problem.\n",
        "\n",
        "Solutions to the vanishing gradient problem\n",
        "In case of exploding gradient, you can:\n",
        "\n",
        "*    stop backpropagating after a certain point, which is usually not optimal because not all of the weights get updated;\n",
        "*    penalize or artificially reduce gradient;\n",
        "*    put a maximum limit on a gradient.\n",
        "\n",
        "\n",
        "In case of vanishing gradient, you can:\n",
        "\n",
        "*    initialize weights so that the potential for vanishing gradient is minimized;\n",
        "*    have Echo State Networks that are designed to solve the vanishing gradient problem;\n",
        "*    have Long Short-Term Memory Networks (LSTMs).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DymB_Dt3NU9"
      },
      "source": [
        "### Advantages of Recurrent Neural Network\n",
        "\n",
        "-    RNN can model sequence of data so that each sample can be assumed to be dependent on previous ones\n",
        "-    Recurrent neural network are even used with convolutional layers to extend the effective pixel neighbourhood.\n",
        "\n",
        "### Disadvantages of Recurrent Neural Network\n",
        "\n",
        "-   Gradient vanishing and exploding problems.\n",
        "-   Training an RNN is a very difficult task.\n",
        "-   It cannot process very long sequences if using tanh or relu as an activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9dfwmUj3NU9"
      },
      "source": [
        "# Building Classifers using the Reuters Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCM4qKF5RVvO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uuEY6rED6y-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQYO3L6ARVvW"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Artificial_Intelligence_Engineering/nlp/RNN and LSTM/reviews.txt', 'r') as f:\n",
        "    reviews = f.read()\n",
        "with open('/content/drive/MyDrive/Artificial_Intelligence_Engineering/nlp/RNN and LSTM/labels.txt', 'r') as f:\n",
        "    labels = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efSB76FERVvn"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "The first step when building a neural network model is getting your data into the proper form to feed into the network. Since we're using embedding layers, we'll need to encode each word with an integer. We'll also want to clean it up a bit.\n",
        "\n",
        "You can see an example of the reviews data above. We'll want to get rid of those periods. Also, you might notice that the reviews are delimited with newlines `\\n`. To deal with those, I'm going to split the text into each review using `\\n` as the delimiter. Then I can combined all the reviews back together into one big string.\n",
        "\n",
        "First, let's remove all punctuation. Then get all the text without the newlines and split it into individual words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGxPt5v3qfyz"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
        "reviews = all_text.split('\\n')\n",
        "print(len(reviews))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEw2HAeTp5L2"
      },
      "outputs": [],
      "source": [
        "words = []\n",
        "for sen in  reviews:\n",
        "  for word in sen.split():\n",
        "    words.append(word)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "id": "boKUiR3J9Y0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiiFrUTfoVJh"
      },
      "outputs": [],
      "source": [
        "len(reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6r5c_IjRVvo"
      },
      "outputs": [],
      "source": [
        "all_text = ' '.join(reviews)\n",
        "words = all_text.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmno6G-qqMst"
      },
      "outputs": [],
      "source": [
        "len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n_27QUtRVv6"
      },
      "source": [
        "### Encoding the words\n",
        "\n",
        "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our reviews into integers so they can be passed into the network.\n",
        "\n",
        "Now you're going to encode the words with integers. Build a dictionary that maps words to integers. Later we're going to pad our input vectors with zeros, so make sure the integers **start at 1, not 0**.\n",
        "\n",
        "Also, convert the reviews to integers and store the reviews in a new list called `reviews_ints`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zEIjQwkzzlDK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuahlcNJsZDc"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "counts = Counter(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVXsGnRHsdj1",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90IsZkI2tIli"
      },
      "outputs": [],
      "source": [
        "sorted_dict = {k: v for k, v in sorted(counts.items(), key=lambda item: item[1],reverse=True)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMNAqJUwvxFF",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "sorted_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRF9Sejqvkrs",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(reviews)"
      ],
      "metadata": {
        "id": "SkjKb77gNXdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews[0]"
      ],
      "metadata": {
        "id": "8plK2AnzNaby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uQ2tFTd3zbGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo2c7s8lRVv7"
      },
      "outputs": [],
      "source": [
        "# Create your dictionary that maps vocab words to integers here\n",
        "vocab_to_int = {word: i for i, word in enumerate(vocab, 1)} # start from 1\n",
        "\n",
        "# Convert the reviews to integers, same shape as reviews list, but with integers\n",
        "review_ints = []\n",
        "for each in reviews:\n",
        "    review_ints.append([vocab_to_int[word] for word in each.split()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4tKV6zcRVwB"
      },
      "source": [
        "### Encoding the labels\n",
        "\n",
        "Our labels are \"positive\" or \"negative\". To use these labels in our network, we need to convert them to 0 and 1.\n",
        "\n",
        "Convert labels from `positive` and `negative` to 1 and 0, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGjEXEvh3NVQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "labels = labels.split('\\n')\n",
        "labels = [1 if label == 'positive' else 0 for label in labels]\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw7Kq0M7x-gS"
      },
      "outputs": [],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm3R3YdnRVwH"
      },
      "source": [
        "If you built `labels` correctly, you should see the next output."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(review_ints))\n",
        "print(len(labels))"
      ],
      "metadata": {
        "id": "-j51CW7q4sw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_of_list = []\n",
        "for lst in review_ints:\n",
        "  len_of_list.append(len(lst))\n"
      ],
      "metadata": {
        "id": "YzqfABR65W75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_of_list"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7Er6nAZ951qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min(len_of_list)"
      ],
      "metadata": {
        "id": "mMmUrR4y5q2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmin(len_of_list)"
      ],
      "metadata": {
        "id": "sRhX40vN51RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFxk4Ib2RVwH"
      },
      "outputs": [],
      "source": [
        "review_lens = Counter([len(x) for x in review_ints])\n",
        "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
        "print(\"Maximum review length: {}\".format(max(review_lens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI834Hex2GkI"
      },
      "outputs": [],
      "source": [
        "print(len(review_ints))\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H_Isncy2PlK",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "len_list = [len(i) for i in review_ints]\n",
        "# np.argmin(len_list)\n",
        "len_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmin(len_list)"
      ],
      "metadata": {
        "id": "etDACHZ5OszA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLKm5ehp2sAq"
      },
      "outputs": [],
      "source": [
        "review_ints = review_ints[:-1]\n",
        "labels = labels[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV-8JSB029r8"
      },
      "outputs": [],
      "source": [
        "print(len(review_ints))\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(review_ints[1]))\n",
        "print(len(review_ints[2]))\n",
        "print(len(review_ints[3]))"
      ],
      "metadata": {
        "id": "GbqIVWo61J9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4f6LZVBRVwO"
      },
      "source": [
        "Okay, a couple issues here. We seem to have one review with zero length. And, the maximum review length is way too many steps for our RNN. Let's truncate to 200 steps. For reviews shorter than 200, we'll pad with 0s. For reviews longer than 200, we can truncate them to the first 200 characters.\n",
        "\n",
        "First, remove the review with zero length from the `reviews_ints` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EEulHd_RVwR"
      },
      "outputs": [],
      "source": [
        "# Filter out that review with 0 length\n",
        "# review_ints = [review for review in review_ints if (len(review) > 0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TsZFRJhztvw"
      },
      "outputs": [],
      "source": [
        "len(review_ints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awsg5nCcRVwZ"
      },
      "source": [
        "Now, create an array `features` that contains the data we'll pass to the network. The data should come from `review_ints`, since we want to feed integers to the network. Each row should be 200 elements long. For reviews shorter than 200 words, left pad with 0s. That is, if the review is `['best', 'movie', 'ever']`, `[117, 18, 128]` as integers, the row will look like `[0, 0, 0, ..., 0, 117, 18, 128]`. For reviews longer than 200, use on the first 200 words as the feature vector.\n",
        "\n",
        "This isn't trivial and there are a bunch of ways to do this. But, if you're going to be building your own deep learning networks, you're going to have to get used to preparing your data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9guxlCxH3NVU"
      },
      "outputs": [],
      "source": [
        "review_ints[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iXbofAT2RVwa"
      },
      "outputs": [],
      "source": [
        "seq_len = 200\n",
        "features = []\n",
        "for review in review_ints:\n",
        "    review_len = len(review)\n",
        "    len_diff = seq_len - review_len\n",
        "    if len_diff <= 0:\n",
        "        features.append(review[:seq_len])\n",
        "\n",
        "    else:\n",
        "        padding = [0] * len_diff\n",
        "        padded_feature = padding + review\n",
        "        features.append(padded_feature)\n",
        "\n",
        "features = np.asarray(features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(features[0]))\n",
        "print(len(features[1]))\n",
        "print(len(features[2]))"
      ],
      "metadata": {
        "id": "ZGu-QCDb7OxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inlsPUdsRVwh"
      },
      "source": [
        "## Training, Validation, Test\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yakhF3zZRVwi"
      },
      "source": [
        "With our data in nice shape, we'll split it into training, validation, and test sets.\n",
        "\n",
        "Create the training, validation, and test sets here. You'll need to create sets for the features and the labels, `train_x` and `train_y` for example. Define a split fraction, `split_frac` as the fraction of data to keep in the training set. Usually this is set to 0.8 or 0.9. The rest of the data will be split in half to create the validation and testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8TJf7t4RVwj"
      },
      "outputs": [],
      "source": [
        "split_frac = 0.8\n",
        "split_idx = int(len(features) * split_frac)\n",
        "\n",
        "train_x, val_x = features[:split_idx], features[split_idx:]\n",
        "train_y, val_y = labels[:split_idx], labels[:split_idx]\n",
        "\n",
        "test_idx = int(len(val_x) * 0.5)\n",
        "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
        "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
        "\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PQp7oPt3NVX"
      },
      "source": [
        "## Simple RNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikeras"
      ],
      "metadata": {
        "id": "rZCbLWB68uLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BS94sNQB3NVX",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.datasets import reuters\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, Activation\n",
        "from keras import optimizers\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "# from keras.wrappers import KerasClassifier\n",
        "\n",
        "# parameters for data load\n",
        "num_words = 30000\n",
        "maxlen = 50\n",
        "test_split = 0.3\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words = num_words, maxlen = maxlen, test_split = test_split)\n",
        "\n",
        "\n",
        "\n",
        "# pad the sequences with zeros\n",
        "# padding parameter is set to 'post' => 0's are appended to end of sequences\n",
        "X_train = pad_sequences(X_train, padding = 'post')\n",
        "X_test = pad_sequences(X_test, padding = 'post')\n",
        "\n",
        "X_train = np.array(X_train).reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = np.array(X_test).reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "y_data = np.concatenate((y_train, y_test))\n",
        "y_data = to_categorical(y_data)\n",
        "y_train = y_data[:1395]\n",
        "y_test = y_data[1395:]\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(50, input_shape = (49,1), return_sequences = False))\n",
        "model.add(Dense(46))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "adam = optimizers.Adam(lr = 0.001)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# model = KerasClassifier(build_fn = vanilla_rnn, epochs = 200, batch_size = 50, verbose = 1)\n",
        "model.fit(X_train, y_train,epochs = 200,batch_size = 50, verbose = 1)\n",
        "\n",
        "# y_pred = model.predict(X_test)\n",
        "# y_test_ = np.argmax(y_test, axis = 1)\n",
        "\n",
        "# print(accuracy_score(y_pred, y_test_))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.shape"
      ],
      "metadata": {
        "id": "CxyCiAUdwgtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_x.reshape(20000,200,1)"
      ],
      "metadata": {
        "id": "u3URlMIRxQ0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Oy1uhqz3NVY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
        "#from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, Activation\n",
        "from keras import optimizers\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "# model.add(SimpleRNN(50, input_shape = (200,1), return_sequences = True))\n",
        "# model.add(SimpleRNN(50,return_sequences = True))\n",
        "# model.add(SimpleRNN(50))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(46))\n",
        "model.add(Dense(46))\n",
        "model.add(Dense(46))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "adam = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
        "\n",
        "model.fit(train_x, train_y, epochs = 1, batch_size = 50, verbose = 1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2l4lDPY2o1E"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(test_x)\n",
        "#y_test_ = np.argmax(test_y,)\n",
        "\n",
        "print(accuracy_score(y_pred, val_y))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}